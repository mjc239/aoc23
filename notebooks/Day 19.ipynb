{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add module to path\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from aoc23.utils import read_input\n",
    "from dataclasses import dataclass\n",
    "import operator\n",
    "from dataclasses import field\n",
    "from copy import deepcopy\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_19 = read_input(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the [day 19 puzzle](https://adventofcode.com/2023), we are given a collection of __workflows__, describing how to sort through a collection of __parts__. Here is how the puzzle describes these workflows:\n",
    "\n",
    ">Each workflow has a name and contains a list of rules; each rule specifies a condition and where to send the part if the condition is true. The first rule that matches the part being considered is applied immediately, and the part moves on to the destination described by the rule. (The last rule in each workflow has no condition and always applies if reached.)\n",
    "\n",
    "Each part has a value assigned to each of the four attributes `x`, `m`, `a` and `s`, which are referenced by the rules in the workflows (I will call these `xmas` attributes). Here is the example set of workflows provided to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = [\n",
    "    'px{a<2006:qkq,m>2090:A,rfg}',\n",
    "    'pv{a>1716:R,A}',\n",
    "    'lnx{m>1548:A,A}',\n",
    "    'rfg{s<537:gd,x>2440:R,A}',\n",
    "    'qs{s>3448:A,lnx}',\n",
    "    'qkq{x<1416:A,crn}',\n",
    "    'crn{x>2662:A,R}',\n",
    "    'in{s<1351:px,qqz}',\n",
    "    'qqz{s>2770:qs,m<1801:hdj,R}',\n",
    "    'gd{a>3333:R,R}',\n",
    "    'hdj{m>838:A,pv}',\n",
    "    '',\n",
    "    '{x=787,m=2655,a=1222,s=2876}',\n",
    "    '{x=1679,m=44,a=2067,s=496}',\n",
    "    '{x=2036,m=264,a=79,s=2244}',\n",
    "    '{x=2461,m=1339,a=466,s=291}',\n",
    "    '{x=2127,m=1623,a=2188,s=1013}'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final steps of the workflows are always either an `A` (accepted) or `R` (rejected) - by following taking a part and moving from workflow to workflow, we will always end up in one of these two states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to represent the objects of interest better, let's define some classes of objects; firstly, a `Part` object which encapsulates all the properties of a part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Part:\n",
    "    x: int\n",
    "    m: int\n",
    "    a: int\n",
    "    s: int\n",
    "        \n",
    "    @classmethod\n",
    "    def create(cls, def_string):\n",
    "        # Create a part from the string provided in the input\n",
    "        attributes = def_string.strip('{}').split(',')\n",
    "        attributes = [a.split('=') for a in attributes]\n",
    "        attribute_dict = {a[0]: int(a[1]) for a in attributes}\n",
    "        \n",
    "        return cls(**attribute_dict)\n",
    "    \n",
    "    @property\n",
    "    def part_sum(self):\n",
    "        # Part value = sum of xmas attributes\n",
    "        return self.x + self.m + self.a + self.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Part(x=787, m=2655, a=1222, s=2876)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstration\n",
    "Part.create('{x=787,m=2655,a=1222,s=2876}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, define a `Condition`, which represents each specific condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Condition:\n",
    "    variable: str # e.g. x\n",
    "    relation: str # e.g. <\n",
    "    value: int    # e.g. 101\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.variable}{self.relation}{self.value}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x<101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstration\n",
    "Condition('x', '<', 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first job is the process the puzzle inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_workflow_string(workflow_string: str):\n",
    "    # input = string of form:\n",
    "    # 'px{a<2006:qkq,m>2090:A,rfg}'\n",
    "    \n",
    "    # name: 'px'\n",
    "    # rules_string: 'a<2006:qkq,m>2090:A,rfg'\n",
    "    name, rules_string = workflow_string[:-1].split('{')\n",
    "    \n",
    "    # processed_rules: [[a<2006, 'qkq'], [m>2090, 'A'], ['rfg']]\n",
    "    processed_rules = []\n",
    "    \n",
    "    for rule_string in rules_string.split(','):\n",
    "        # rule_string of form:\n",
    "        # 'a<2006:qkq' or 'rfg'\n",
    "        rule = rule_string.split(':')\n",
    "        \n",
    "        if len(rule) == 1:\n",
    "            # final rule in workflow\n",
    "            processed_rules.append(rule)\n",
    "            \n",
    "        elif len(rule) == 2:\n",
    "            # Convert condition to Condition\n",
    "            condition_string, new_name = rule\n",
    "            \n",
    "            # condition_string: 'a<2006'\n",
    "            processed_rules.append([\n",
    "                Condition(condition_string[0],\n",
    "                          condition_string[1],\n",
    "                          int(condition_string[2:])),\n",
    "                new_name\n",
    "            ])\n",
    "    \n",
    "    return name, processed_rules\n",
    "\n",
    "\n",
    "def process_input(input_list: list[str]):\n",
    "    # Split input_list at the empty line\n",
    "    workflows = {}\n",
    "    parts = []\n",
    "    gap_index = input_list.index('')\n",
    "    input_workflows = input_list[:gap_index]\n",
    "    input_parts = input_list[gap_index+1:]\n",
    "    \n",
    "    # Process rules and parts separately\n",
    "    workflows = {name: processed_rules \n",
    "                 for name, processed_rules \n",
    "                 in map(process_workflow_string, input_workflows)}\n",
    "    \n",
    "    parts = [Part.create(line) for line in input_parts]\n",
    "    \n",
    "    return workflows, parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflows and parts are now represented in a much more useful form, as can be seen here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Part(x=787, m=2655, a=1222, s=2876),\n",
       " Part(x=1679, m=44, a=2067, s=496),\n",
       " Part(x=2036, m=264, a=79, s=2244),\n",
       " Part(x=2461, m=1339, a=466, s=291),\n",
       " Part(x=2127, m=1623, a=2188, s=1013)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_workflows, test_parts = process_input(test_input)\n",
    "test_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'px': [[a<2006, 'qkq'], [m>2090, 'A'], ['rfg']],\n",
       " 'pv': [[a>1716, 'R'], ['A']],\n",
       " 'lnx': [[m>1548, 'A'], ['A']],\n",
       " 'rfg': [[s<537, 'gd'], [x>2440, 'R'], ['A']],\n",
       " 'qs': [[s>3448, 'A'], ['lnx']],\n",
       " 'qkq': [[x<1416, 'A'], ['crn']],\n",
       " 'crn': [[x>2662, 'A'], ['R']],\n",
       " 'in': [[s<1351, 'px'], ['qqz']],\n",
       " 'qqz': [[s>2770, 'qs'], [m<1801, 'hdj'], ['R']],\n",
       " 'gd': [[a>3333, 'R'], ['R']],\n",
       " 'hdj': [[m>838, 'A'], ['pv']]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to define a helper function which, when given a part and a wokflow, finds the string which is outputted by following the part down the workflow (either a new workflow name, or the terminal `A` or `R` state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The operator module helps with this:\n",
    "OPERATORS = {\n",
    "        '<': operator.lt,\n",
    "        '>': operator.gt\n",
    "    }\n",
    "\n",
    "def evaluate_workflow(part: Part, workflow: list[list[Condition, str]]) -> str:\n",
    "    # Follow the part along the tree, until an output string is found\n",
    "    # (either a new workflow to move to, or 'A' or 'R')\n",
    "    for rule in workflow:\n",
    "        if len(rule) == 1:\n",
    "            # At end of workflow\n",
    "            return rule[0]\n",
    "        else:\n",
    "            # Check if condition is satisfied\n",
    "            condition, name = rule\n",
    "            operator = OPERATORS[condition.relation]\n",
    "            if operator(getattr(part, condition.variable), condition.value):\n",
    "                return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part: Part(x=787, m=2655, a=1222, s=2876)\n",
      "Rule: [[s<1351, 'px'], ['qqz']]\n",
      "Output: qqz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstration\n",
    "part = Part(x=787, m=2655, a=1222, s=2876)\n",
    "rule = test_workflows['in']\n",
    "print(f'''\n",
    "Part: {part}\n",
    "Rule: {rule}\n",
    "Output: {evaluate_workflow(part, rule)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each part can repeatedly fed into the workflows (starting with the `in` workflow), until one of the terminal states is obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_part_accepted(part: Part, workflows):\n",
    "    # Start at `in`\n",
    "    key = 'in'\n",
    "    \n",
    "    # Continue until termination\n",
    "    while key not in ['A', 'R']:\n",
    "        key = evaluate_workflow(part, workflows[key])\n",
    "        \n",
    "    return key == 'A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part, we are asked only to look at the parts which are accepted, and find the sum of the part attributes - let's check that the value found for the test case matches the expected sum of part sums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "sum([part.part_sum for part in test_parts if is_part_accepted(part, test_workflows)]) == 19114\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this for the full set of parts and workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflows, parts = process_input(input_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368523"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([part.part_sum for part in parts if is_part_accepted(part, workflows)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the answer to part 1 is: __368523__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are told to consider _all_ parts with `xmas` attributes in the range $[1, 4000]$ - this gives a total of $4000^4=2.56\\times10^{14}$ different possible parts, far more than can be considered one at a time using the method from part 1. So we need to be a bit smarter. Instead of considering each part individually, start by considering all of the parts simultaneously, and working out how many are ruled out by each rule that is applied. Specifically, we will track a `Region`, which records the minimum and maximum `xmas` attributes considered along a particular path through the workflows. First lets define the region as a `Region` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Region:\n",
    "    accepted: bool = None\n",
    "        \n",
    "    # Each range starts with the full set of possible values\n",
    "    x: tuple[int] = field(default_factory=lambda: [1, 4000])\n",
    "    m: tuple[int] = field(default_factory=lambda: [1, 4000])\n",
    "    a: tuple[int] = field(default_factory=lambda: [1, 4000])\n",
    "    s: tuple[int] = field(default_factory=lambda: [1, 4000])\n",
    "        \n",
    "    def set_upper_limit(self, variable: str, value: int):\n",
    "        limits = getattr(self, variable)\n",
    "        if value < limits[1]:\n",
    "            limits[1] = value\n",
    "        \n",
    "        # Check that lower limit < upper limit\n",
    "        assert limits[0] <= limits[1]\n",
    "\n",
    "            \n",
    "    def set_lower_limit(self, variable: str, value: int):\n",
    "        limits = getattr(self, variable)\n",
    "        if value > limits[0]:\n",
    "            limits[0] = value\n",
    "        \n",
    "        # Check that lower limit < upper limit\n",
    "        assert limits[0] <= limits[1]\n",
    "        \n",
    "    def count_parts(self):\n",
    "        if len(self.x)*len(self.m)*len(self.a)*len(self.s) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return (self.x[1] - self.x[0] + 1) * \\\n",
    "               (self.m[1] - self.m[0] + 1) * \\\n",
    "               (self.a[1] - self.a[0] + 1) * \\\n",
    "               (self.s[1] - self.s[0] + 1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the ranges for the `xmas` attributes, the `Region` class also knows if the region is accepted or rejected via the `accepted` attribute (which will be `None` until a terminal state of the workflows is found). It has methods for updating the attribute ranges, when given new upper or lower limits for the attributes (e.g. by the application of a specific workflow condition), and also a method for computing the total number of parts that have attribute values in the allowed ranges.\n",
    "\n",
    "Now, in order to follow all of the possible paths through the workflows, set up a stack which will track the `(workflow_name, region)` states that still need to be processed. We follow a depth first search (DFS) approch: at each step, take the most recent state, and compute the change to the region's `xmas` attribute ranges when the workflow rules are followed. Specifically, after each rule two states are created - one which considers the path where the condition is satisfied, and another which considers the condition not satisfied. The region ranges are updated accordingly for each of these states, before being added to the stack, unless a terminal state is found (one with name `A` or `R`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regions(workflows: dict[str, list[list[Condition, str]]]) -> list[Region]:\n",
    "    regions = []\n",
    "    \n",
    "    # Start with a single, maximal range at the 'in' workflow\n",
    "    states = [('in', Region())]\n",
    "    \n",
    "    while len(states) != 0:\n",
    "        # Take the most recently added state\n",
    "        state = states.pop()\n",
    "        name, region = state\n",
    "\n",
    "        workflow = workflows[name]\n",
    "\n",
    "        for rule in workflow:\n",
    "            if len(rule) == 2:\n",
    "                condition, name = rule\n",
    "                # There are 2 cases - the rule is/is not satified\n",
    "                # Make a copy for later\n",
    "                skip_region = deepcopy(region)\n",
    "                \n",
    "                # Case 1 - condition satisfied\n",
    "                # Reset the limits of the region, based on the condition\n",
    "                if condition.relation == '<':\n",
    "                    region.set_upper_limit(condition.variable, condition.value-1)\n",
    "                elif condition.relation == '>':\n",
    "                    region.set_lower_limit(condition.variable, condition.value+1)\n",
    "                \n",
    "                # Add the completed region to regions, \n",
    "                # or add new state to stack\n",
    "                if name in ['A', 'R']:\n",
    "                    region.accepted = name\n",
    "                    regions.append(region)\n",
    "                else:\n",
    "                    states.append((name, region))\n",
    "                \n",
    "                # Case 2 - condition not satisfied\n",
    "                if condition.relation == '>':\n",
    "                    skip_region.set_upper_limit(condition.variable, condition.value)\n",
    "                elif condition.relation == '<':\n",
    "                    skip_region.set_lower_limit(condition.variable, condition.value)\n",
    "                \n",
    "                # Redefine region, to use as input for next rule in loop\n",
    "                region = skip_region\n",
    "\n",
    "            else:\n",
    "                # End of workflow\n",
    "                # Either terminate or add new state to stack\n",
    "                name = rule[0]\n",
    "                if name in ['A', 'R']:\n",
    "                    region.accepted = name\n",
    "                    regions.append(region)\n",
    "                else:\n",
    "                    states.append((name, region))\n",
    "                    \n",
    "    return regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on the test workflows, and see what regions are produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Region(accepted='R', x=[1, 4000], m=[1801, 4000], a=[1, 4000], s=[1351, 2770]),\n",
       " Region(accepted='A', x=[1, 4000], m=[839, 1800], a=[1, 4000], s=[1351, 2770]),\n",
       " Region(accepted='R', x=[1, 4000], m=[1, 838], a=[1717, 4000], s=[1351, 2770]),\n",
       " Region(accepted='A', x=[1, 4000], m=[1, 838], a=[1, 1716], s=[1351, 2770]),\n",
       " Region(accepted='A', x=[1, 4000], m=[1, 4000], a=[1, 4000], s=[3449, 4000]),\n",
       " Region(accepted='A', x=[1, 4000], m=[1549, 4000], a=[1, 4000], s=[2771, 3448]),\n",
       " Region(accepted='A', x=[1, 4000], m=[1, 1548], a=[1, 4000], s=[2771, 3448]),\n",
       " Region(accepted='A', x=[1, 4000], m=[2091, 4000], a=[2006, 4000], s=[1, 1350]),\n",
       " Region(accepted='R', x=[2441, 4000], m=[1, 2090], a=[2006, 4000], s=[537, 1350]),\n",
       " Region(accepted='A', x=[1, 2440], m=[1, 2090], a=[2006, 4000], s=[537, 1350]),\n",
       " Region(accepted='R', x=[1, 4000], m=[1, 2090], a=[3334, 4000], s=[1, 536]),\n",
       " Region(accepted='R', x=[1, 4000], m=[1, 2090], a=[2006, 3333], s=[1, 536]),\n",
       " Region(accepted='A', x=[1, 1415], m=[1, 4000], a=[1, 2005], s=[1, 1350]),\n",
       " Region(accepted='A', x=[2663, 4000], m=[1, 4000], a=[1, 2005], s=[1, 1350]),\n",
       " Region(accepted='R', x=[1416, 2662], m=[1, 4000], a=[1, 2005], s=[1, 1350])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_regions = compute_regions(test_workflows)\n",
    "test_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the workflows are well-defined, each region will have `A` or `R` as its `accepted` attribute. Also, regions should form a __partition__ of the full set of possible parts - in other words:\n",
    "- Each possible part belongs to one of the regions (either accepted or rejected)\n",
    "- No part belongs to more than one region.\n",
    "Therefore, to compute the total number of accepted parts, we just need to add the number of parts in each accepted component of the partition. Within each partition, the number of parts is the product of the widths of each of the attribute ranges (plus 1, to account for the endpoints) - this is implemented on the `Region` class directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the regions do in fact form a partition - first, check that the total number of parts (either accepted or rejected) is equal to the the full number of possible parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parts(regions: list[Region], state: str):\n",
    "    regions = [region for region in regions if region.accepted == state]\n",
    "    return sum([region.count_parts() for region in regions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct number of parts!\n"
     ]
    }
   ],
   "source": [
    "assert count_parts(test_regions, 'A') + count_parts(test_regions, 'R') \\\n",
    "       == 4000**4\n",
    "print('Correct number of parts!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, check that each of the the regions has an empty intersection. Usefully, as each of the regions defines a rectangular region in 4-dimension `xmas`-space, the intersection will also be a 4-dimensional rectangular region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _intersect_intervals(interval_1: list[int], interval_2: list[int]) -> list[int]:\n",
    "    a_1, b_1 = interval_1\n",
    "    a_2, b_2 = interval_2\n",
    "    if b_1 < a_2 or b_2 < a_1:\n",
    "        return []\n",
    "    else:\n",
    "        return [max(a_1, a_2), min(b_1, b_2)]\n",
    "\n",
    "def intersect_regions(region_1: Region, region_2: Region) -> Region:\n",
    "    return Region(\n",
    "        x=_intersect_intervals(region_1.x, region_2.x),\n",
    "        m=_intersect_intervals(region_1.m, region_2.m),\n",
    "        a=_intersect_intervals(region_1.a, region_2.a),\n",
    "        s=_intersect_intervals(region_1.s, region_2.s),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider all combinations of two distinct regions from the full collection of regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region_1, region_2 in combinations(regions, 2):\n",
    "    intersection = intersect_regions(regions[0], regions[1])\n",
    "    assert intersection.count_parts() == 0\n",
    "print('All intersections are empty!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proves that the regions have indeed partitioned the set of all possible parts. Therefore, we can use the sum of the sizes of the individual components of the partitions to give the total number of accepted parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are told how many acceptable parts to expect for the provided test workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert count_parts(test_regions, 'A') == 167409079868000\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's apply this method to the actual workflows, and find the total number of acceptable parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124167549767307"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions = compute_regions(workflows)\n",
    "count_parts(regions, 'A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the answer to part 2 is: __124167549767307__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoc23",
   "language": "python",
   "name": "aoc23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
